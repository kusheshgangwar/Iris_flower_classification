# Iris_flower_classification

ðŸŒŸ Project Spotlight: Mastering Iris Flower Classification with Machine Learning ðŸŒ¸

Project Goal: The primary objective was to develop a machine learning model capable of learning from the distinct characteristics of Iris flowers (sepal length, sepal width, petal length, and petal width) and accurately classifying them into their respective species. This automation has significant potential for applications in botany, horticulture, and environmental monitoring.

My Approach & Key Steps:

1. Data Acquisition & Initial Understanding: * I began by loading the Iris.csv dataset, which contains 150 entries and 6 columns, including the 'Id', four measurement features, and the 'Species' label. * A quick df.head() provided an immediate glimpse into the data structure. * Using df.shape and df.info(), I confirmed the dataset's dimensions and data types, noting that all features were numerical (float64 or int64) and the 'Species' column was an object type, indicating categorical data.

2. Data Quality Check & Preprocessing: * Duplicate Values: A crucial step was checking for duplicate entries using df.duplicated().sum(). I was pleased to find 0 duplicate rows, indicating a clean dataset. * Missing Values: Similarly, df.isnull().sum() confirmed no missing values across any column, ensuring data completeness. * Understanding Variables: I used df.describe(include='all').round(2) to get a statistical summary of both numerical and categorical features. This revealed key insights like the distribution of measurements and the equal frequency (50 each) of the three Iris species. * Unique Values: Iterating through each column with df[i].nunique() helped confirm the distinct values for each feature, especially noting the 3 unique species. * Data Wrangling: The 'Id' column was irrelevant for classification, so I dropped it using data = df.iloc[:, 1:], creating a refined dataset for modeling.

3. Exploratory Data Analysis (EDA) & Visualization: * Distribution of Numerical Variables: I created a 2x2 grid of histograms to visualize the distribution of 'Sepal Length', 'Sepal Width', 'Petal Length', and 'Petal Width'. This provided a clear understanding of the spread and central tendency of each measurement. * Feature Relationships & Species Separation: * Sepal Length vs. Sepal Width: A scatter plot, color-coded by species, revealed initial clusters, though some overlap was visible, particularly between 'Iris-versicolor' and 'Iris-virginica'. * Petal Length vs. Petal Width: This scatter plot showed much clearer separation between the three species, indicating that petal measurements are highly discriminative. 'Iris-setosa' was distinctly separated from the other two. * Sepal Length vs. Petal Length: Another scatter plot highlighted the strong positive correlation between these two features and their ability to differentiate species. * Sepal Width vs. Petal Width: This plot also contributed to understanding the feature interplay in species identification. * Correlation Heatmap: A heatmap of the correlation matrix (excluding 'Species') visually represented the linear relationships between the numerical features. This confirmed strong positive correlations between 'Petal Length' and 'Petal Width', and 'Sepal Length' and 'Petal Length', which are valuable insights for model building.

4. Feature Engineering & Data Pre-processing: * Categorical Encoding: The 'Species' column, being categorical, needed to be converted into numerical format for machine learning algorithms. I used LabelEncoder from sklearn.preprocessing to transform 'Iris-setosa', 'Iris-versicolor', and 'Iris-virginica' into 0, 1, and 2 respectively. * Defining X and Y: I separated the features (x = data.drop(columns=['Species'], axis=1)) from the target variable (y = data['Species']). * Data Splitting: To ensure unbiased model evaluation, I split the dataset into training and testing sets using train_test_split with a test_size=0.3. I also verified the distribution of species in the training set to ensure representativeness.

5. Machine Learning Model Implementation: Logistic Regression * I chose Logistic Regression as my initial classification model, a powerful and interpretable algorithm for multi-class classification. * I implemented a custom evaluate_model function to standardize the evaluation process across different models. This function: * Fits the model to the training data. * Makes predictions on both training and testing sets. * Generates and visualizes Confusion Matrices for both train and test sets, providing a clear picture of correct and incorrect classifications. * Prints detailed Classification Reports (including precision, recall, and F1-score) for each class and overall weighted averages, offering a comprehensive performance breakdown. * Calculates and returns key metrics like precision, recall, accuracy, and F1-score for both train and test sets.

Initial Results (Logistic Regression): (Once you run the Logistic Regression model in your notebook, you can insert the actual performance metrics here. For example, if your accuracy is 97% on test data, you can mention that.) The Logistic Regression model demonstrated strong performance, achieving impressive accuracy and F1-scores on both the training and unseen test data, indicating its effectiveness in classifying Iris species.

This project has been an invaluable learning experience, solidifying my understanding of the end-to-end machine learning workflow. I'm excited to continue exploring other classification algorithms and optimization techniques for this dataset!
